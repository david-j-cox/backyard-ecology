{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5347e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- requirements: requests, pandas, numpy, scipy, scikit-learn ---\n",
    "# pip install requests pandas numpy scipy scikit-learn\n",
    "# (optional) pip install tqdm\n",
    "\n",
    "import sys, math, time, itertools, datetime as dt, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import linregress\n",
    "import requests\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Optional pretty progress\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    HAS_TQDM = True\n",
    "except Exception:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "GRAPHQL_URL = \"https://app.birdweather.com/graphql\"\n",
    "\n",
    "# Duval County, FL rough bounding box (WGS84); tweak if desired\n",
    "BBOX = {\n",
    "    \"ne\": {\"lat\": 30.60, \"lon\": -81.30},\n",
    "    \"sw\": {\"lat\": 30.10, \"lon\": -82.10},\n",
    "}\n",
    "\n",
    "# Date window (inclusive start, up to today by default)\n",
    "START_DATE = \"2017-01-01\"\n",
    "END_DATE = dt.date.today().isoformat()\n",
    "\n",
    "# Chunk size for station queries to daily counts\n",
    "STATION_CHUNK = 50\n",
    "\n",
    "# -----------------------\n",
    "# Minimal progress helper\n",
    "# -----------------------\n",
    "class Progress:\n",
    "    def __init__(self, use_tqdm: bool = HAS_TQDM):\n",
    "        self.use_tqdm = use_tqdm\n",
    "\n",
    "    def bar(self, total: int, desc: str):\n",
    "        if self.use_tqdm:\n",
    "            return tqdm(total=total, desc=desc, unit=\"step\", leave=True)\n",
    "        # Fallback minimal bar API\n",
    "        class _Bar:\n",
    "            def __init__(self, total, desc):\n",
    "                self.total = total\n",
    "                self.n = 0\n",
    "                self.desc = desc\n",
    "                print(f\"{desc} (0/{total})\")\n",
    "            def update(self, k=1):\n",
    "                self.n += k\n",
    "                print(f\"{self.desc} ({self.n}/{self.total})\")\n",
    "            def set_postfix(self, *args, **kwargs):\n",
    "                pass\n",
    "            def close(self):\n",
    "                pass\n",
    "        return _Bar(total, desc)\n",
    "\n",
    "    def note(self, msg: str):\n",
    "        ts = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"[{ts}] {msg}\")\n",
    "\n",
    "progress = Progress()\n",
    "\n",
    "# -----------------------\n",
    "# Requests session with retries\n",
    "# -----------------------\n",
    "def make_session():\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    try:\n",
    "        # Retry is in urllib3; import defensively\n",
    "        from urllib3.util.retry import Retry\n",
    "        retries = Retry(\n",
    "            total=5, backoff_factor=0.5,\n",
    "            status_forcelist=(429, 500, 502, 503, 504),\n",
    "            allowed_methods=frozenset([\"POST\"])\n",
    "        )\n",
    "        s = requests.Session()\n",
    "        s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "        s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "        return s\n",
    "    except Exception:\n",
    "        return requests.Session()\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "# -----------------------\n",
    "# GraphQL helper with basic error surfacing\n",
    "# -----------------------\n",
    "def gql(query: str, variables: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    r = SESSION.post(GRAPHQL_URL, json={\"query\": query, \"variables\": variables}, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    out = r.json()\n",
    "    if \"errors\" in out and out[\"errors\"]:\n",
    "        raise RuntimeError(json.dumps(out[\"errors\"], indent=2))\n",
    "    return out[\"data\"]\n",
    "\n",
    "# -----------------------\n",
    "# Queries\n",
    "# -----------------------\n",
    "STATIONS_QUERY = \"\"\"\n",
    "query stations($first:Int,$after:String,$ne:InputLocation,$sw:InputLocation){\n",
    "  stations(first:$first, after:$after, ne:$ne, sw:$sw){\n",
    "    pageInfo{ endCursor hasNextPage }\n",
    "    nodes{\n",
    "      id\n",
    "      name\n",
    "      country\n",
    "      state\n",
    "      coords { lat lon }\n",
    "      type\n",
    "      timezone\n",
    "      locationPrivacy\n",
    "    }\n",
    "    totalCount\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "DAILY_COUNTS_QUERY = \"\"\"\n",
    "query dailyDetectionCounts($period: InputDuration, $stationIds: [ID!]) {\n",
    "  dailyDetectionCounts(period: $period, stationIds: $stationIds){\n",
    "    date\n",
    "    total\n",
    "    counts { key count }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------\n",
    "# Utils\n",
    "# -----------------------\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "# -----------------------\n",
    "# 1) Discover stations in bbox (with paging progress)\n",
    "# -----------------------\n",
    "def fetch_station_ids(ne, sw, page_size=100):\n",
    "    ids, nodes = [], []\n",
    "    after = None\n",
    "    page_i = 0\n",
    "    progress.note(\"Discovering public BirdWeather stations in bbox...\")\n",
    "    start_t = time.time()\n",
    "\n",
    "    # Unknown total pages; we use totalCount if provided to show %.\n",
    "    while True:\n",
    "        page_i += 1\n",
    "        data = gql(STATIONS_QUERY, {\"first\": page_size, \"after\": after, \"ne\": ne, \"sw\": sw})\n",
    "        batch = data[\"stations\"][\"nodes\"]\n",
    "        nodes.extend(batch)\n",
    "        ids.extend([n[\"id\"] for n in batch])\n",
    "\n",
    "        pi = data[\"stations\"][\"pageInfo\"]\n",
    "        totalCount = data[\"stations\"].get(\"totalCount\", None)\n",
    "\n",
    "        if totalCount is not None and totalCount > 0:\n",
    "            pct = min(100, int(100 * len(ids) / totalCount))\n",
    "            progress.note(f\"Stations page {page_i}: {len(ids)}/{totalCount} (~{pct}%).\")\n",
    "        else:\n",
    "            progress.note(f\"Stations page {page_i}: accumulated {len(ids)} (totalCount unavailable).\")\n",
    "\n",
    "        if not pi[\"hasNextPage\"]:\n",
    "            break\n",
    "        after = pi[\"endCursor\"]\n",
    "\n",
    "    elapsed = time.time() - start_t\n",
    "    progress.note(f\"Station discovery complete: {len(ids)} stations in {elapsed:.1f}s.\")\n",
    "    return ids, pd.DataFrame(nodes)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Pull daily counts with progress + ETA\n",
    "# -----------------------\n",
    "def fetch_daily_counts(station_ids, start_date, end_date, chunk_size=STATION_CHUNK):\n",
    "    daily_frames = []\n",
    "    chunk_list = list(chunks(station_ids, chunk_size))\n",
    "    if not chunk_list:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    bar = progress.bar(total=len(chunk_list), desc=\"Pulling dailyDetectionCounts\")\n",
    "    records_so_far = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, chunk in enumerate(chunk_list, start=1):\n",
    "        t_chunk_start = time.time()\n",
    "\n",
    "        data = gql(DAILY_COUNTS_QUERY, {\"period\": {\"from\": start_date, \"to\": end_date}, \"stationIds\": chunk})\n",
    "        rows = data[\"dailyDetectionCounts\"]\n",
    "\n",
    "        local_count = 0\n",
    "        for r in rows:\n",
    "            date = r[\"date\"]\n",
    "            total = r[\"total\"]\n",
    "            for c in r[\"counts\"]:\n",
    "                daily_frames.append({\n",
    "                    \"date\": date,\n",
    "                    \"speciesId\": c[\"key\"],\n",
    "                    \"count\": c[\"count\"],\n",
    "                    \"total\": total\n",
    "                })\n",
    "                records_so_far += 1\n",
    "                local_count += 1\n",
    "\n",
    "        # Progress/ETA\n",
    "        elapsed = time.time() - t0\n",
    "        done = i\n",
    "        remaining = len(chunk_list) - done\n",
    "        avg_per_chunk = elapsed / max(done, 1)\n",
    "        eta_sec = remaining * avg_per_chunk\n",
    "\n",
    "        if HAS_TQDM:\n",
    "            bar.update(1)\n",
    "            bar.set_postfix({\n",
    "                \"chunk\": f\"{i}/{len(chunk_list)}\",\n",
    "                \"rows\": records_so_far,\n",
    "                \"eta\": f\"{int(eta_sec)}s\"\n",
    "            })\n",
    "        else:\n",
    "            bar.update(1)\n",
    "            progress.note(f\"Chunk {i}/{len(chunk_list)} (+{local_count} rows, {records_so_far} total). \"\n",
    "                          f\"ETA ~{int(eta_sec)}s\")\n",
    "\n",
    "        # Gentle pacing to be kind to the API (tweak or remove if needed)\n",
    "        dt_chunk = time.time() - t_chunk_start\n",
    "        if dt_chunk < 0.2:\n",
    "            time.sleep(0.2 - dt_chunk)\n",
    "\n",
    "    bar.close()\n",
    "    progress.note(f\"Daily counts download complete: {records_so_far} rows in {time.time() - t0:.1f}s.\")\n",
    "    return pd.DataFrame(daily_frames)\n",
    "\n",
    "# -----------------------\n",
    "# 3) Compute metrics and save\n",
    "# -----------------------\n",
    "def compute_and_save_metrics(daily: pd.DataFrame):\n",
    "    progress.note(\"Computing daily/weekly metricsâ€¦\")\n",
    "\n",
    "    daily[\"date\"] = pd.to_datetime(daily[\"date\"])\n",
    "    daily[\"week\"] = daily[\"date\"].dt.to_period(\"W\").apply(lambda r: r.start_time.date())\n",
    "    daily[\"year\"] = daily[\"date\"].dt.year\n",
    "\n",
    "    # County-level totals by day\n",
    "    totals = daily.groupby(\"date\", as_index=False)[\"total\"].max().sort_values(\"date\")\n",
    "\n",
    "    # CV of daily detections\n",
    "    cv_daily = (totals[\"total\"].std(ddof=1) / totals[\"total\"].mean()) if len(totals) > 1 else np.nan\n",
    "\n",
    "    # Richness and Shannon H' per day\n",
    "    richness = daily.groupby(\"date\")[\"speciesId\"].nunique().rename(\"richness\")\n",
    "    day_species = daily.pivot_table(index=\"date\", columns=\"speciesId\",\n",
    "                                    values=\"count\", aggfunc=\"sum\", fill_value=0)\n",
    "    p = day_species.div(day_species.sum(axis=1), axis=0).replace(0, np.nan)\n",
    "    shannon = (-(p * np.log(p)).sum(axis=1)).rename(\"shannon_H\")\n",
    "\n",
    "    metrics = pd.concat([totals.set_index(\"date\"), richness, shannon], axis=1).reset_index()\n",
    "\n",
    "    # Weekly beta diversity: SÃ¸rensen dissimilarity between adjacent weeks\n",
    "    weekly_presence = (daily.groupby([\"week\", \"speciesId\"])[\"count\"]\n",
    "                       .sum().unstack(fill_value=0) > 0).astype(int)\n",
    "    sorensen = []\n",
    "    weeks_sorted = sorted(weekly_presence.index)\n",
    "    for w_prev, w_curr in zip(weeks_sorted[:-1], weeks_sorted[1:]):\n",
    "        a = weekly_presence.loc[w_prev]\n",
    "        b = weekly_presence.loc[w_curr]\n",
    "        a_sum = a.sum(); b_sum = b.sum(); intersection = (a & b).sum()\n",
    "        denom = (a_sum + b_sum)\n",
    "        d_s = np.nan if denom == 0 else 1 - (2 * intersection / denom)\n",
    "        sorensen.append({\"week\": w_curr, \"sorensen_dissimilarity\": d_s})\n",
    "    beta_df = pd.DataFrame(sorensen)\n",
    "\n",
    "    # Simple trend per year on daily totals\n",
    "    trend_rows = []\n",
    "    totals = totals.sort_values(\"date\")\n",
    "    for yr, sub in totals.groupby(totals[\"date\"].dt.year):\n",
    "        if len(sub) > 5:\n",
    "            x = (sub[\"date\"] - sub[\"date\"].min()).dt.days.values\n",
    "            y = sub[\"total\"].values\n",
    "            slope, intercept, r, p, se = linregress(x, y)\n",
    "            trend_rows.append({\n",
    "                \"year\": yr,\n",
    "                \"slope_det_per_day\": slope,\n",
    "                \"r\": r,\n",
    "                \"p\": p\n",
    "            })\n",
    "    trend = pd.DataFrame(trend_rows)\n",
    "\n",
    "    progress.note(\"Writing CSV outputsâ€¦\")\n",
    "    metrics.to_csv(\"duval_daily_metrics.csv\", index=False)\n",
    "    beta_df.to_csv(\"duval_weekly_beta_diversity.csv\", index=False)\n",
    "    trend.to_csv(\"duval_trends_by_year.csv\", index=False)\n",
    "\n",
    "    try:\n",
    "        n_days = metrics[\"date\"].nunique()\n",
    "        n_weeks = beta_df[\"week\"].nunique() if not beta_df.empty else 0\n",
    "        n_species = daily[\"speciesId\"].nunique()\n",
    "        progress.note(\n",
    "            f\"SUMMARY â†’ Days: {n_days} | Weeks: {n_weeks} | Species: {n_species} | Daily CV: {cv_daily:.3f}\"\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return metrics, beta_df, trend, cv_daily\n",
    "\n",
    "# -----------------------\n",
    "# MAIN\n",
    "# -----------------------\n",
    "def main():\n",
    "    # 1) Stations\n",
    "    station_ids, stations_df = fetch_station_ids(BBOX[\"ne\"], BBOX[\"sw\"])\n",
    "    if stations_df.empty or not station_ids:\n",
    "        print(\"No public stations found in bbox. Try expanding the bbox or checking dates.\")\n",
    "        return\n",
    "    stations_df.to_csv(\"duval_stations.csv\", index=False)\n",
    "    progress.note(f\"Saved station manifest â†’ duval_stations.csv\")\n",
    "\n",
    "    # 2) Daily counts\n",
    "    daily = fetch_daily_counts(station_ids, START_DATE, END_DATE, STATION_CHUNK)\n",
    "    if daily.empty:\n",
    "        print(\"No daily data returned for this bbox/period. Try adjusting dates or bbox.\")\n",
    "        return\n",
    "\n",
    "    # 3) Metrics & outputs\n",
    "    compute_and_save_metrics(daily)\n",
    "\n",
    "    progress.note(\"All files saved:\")\n",
    "    print(\"- duval_stations.csv\")\n",
    "    print(\"- duval_daily_metrics.csv\")\n",
    "    print(\"- duval_weekly_beta_diversity.csv\")\n",
    "    print(\"- duval_trends_by_year.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753383b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
